{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "fc84762229d39ec4a408a69977fa9d3f7252cc12285063738cedf924b23369ec"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils import project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_path: str, test_size: int) -> pd.Series:\n",
    "    \"\"\"Splits training dataset for local use\"\"\"\n",
    "    og_train = pd.read_csv(project_root() + train_path, header=None, names=[\"label\", \"node_id\"])\n",
    "    og_train = og_train[[\"node_id\", \"label\"]]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        og_train[\"node_id\"], og_train[\"label\"], test_size=test_size, random_state=2\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'split_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ed1a4e265ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = split_dataset(\"/data/train.csv\", test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sets(train_path: str, text_path: str, nodeid2paper_path: str, test_path: str, test_size: float) -> pd.DataFrame:\n",
    "    \"\"\"Makes a training, local test set and test set\"\"\"\n",
    "    assert test_size < 1, \"Test size must be smaller than 1\"\n",
    "    # reading files\n",
    "    train_df = pd.read_csv(project_root() + train_path, header=None, names=[\"label\", \"node_id\"])\n",
    "    train_df = train_df[[\"node_id\", \"label\"]]\n",
    "    nodeid2paperid = pd.read_csv(project_root() + nodeid2paper_path)\n",
    "    nodeid2paperid.rename(columns={\"node idx\": \"node_id\", \"paper id\": \"paper_id\"}, inplace=True)\n",
    "    text_df = pd.read_csv(project_root() + text_path, header=None, names=[\"paper_id\", \"title\", \"abstract\"])\n",
    "    test_df = pd.read_csv(project_root() + test_path, header=None, names=[\"node_id\"])\n",
    "    # merge paper id\n",
    "    train_df = train_df.merge(nodeid2paperid, on=\"node_id\", how=\"left\")\n",
    "    test_df = test_df.merge(nodeid2paperid, on=\"node_id\", how=\"left\")\n",
    "    # splitting training and testing for local use\n",
    "    training_df = train_df.iloc[:int(len(train_df) - (len(train_df) * test_size)), :]\n",
    "    testing_df = train_df.iloc[int(len(train_df) - (len(train_df) * test_size)):, :]\n",
    "    # training and test set\n",
    "    training_set = text_df.merge(training_df, how=\"inner\", on=\"paper_id\")\n",
    "    training_set = training_set[[\"node_id\", \"title\", \"abstract\", \"label\"]]\n",
    "    local_test_set = text_df.merge(testing_df, how=\"inner\", on=\"paper_id\")\n",
    "    local_test_set = local_test_set[[\"node_id\", \"title\", \"abstract\", \"label\"]]\n",
    "    test_set = text_df.merge(test_df, how=\"inner\", on=\"paper_id\")\n",
    "    test_set = test_set[[\"node_id\", \"title\", \"abstract\"]]\n",
    "    return training_set, local_test_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, local_test_set, test_set = make_sets(\"/data/train.csv\", \"/data/text.csv\", \"/data/nodeid2paperid.csv\", \"/data/test.csv\", 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "training_set.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(54000, 4)\n(54000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "sp  = sparse.rand(54000, 10000)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(training_set.shape)\n",
    "print(sp.shape)\n",
    "# np.hstack((training_set, sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         0    1         2    3    4    5    6    7    8    9  ...  9994  \\\n",
       "0      0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1      0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2      0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3      0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4      0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "...    ...  ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
       "53995  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "53996  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "53997  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "53998  0.0  0.0  0.682094  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "53999  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "           9995  9996  9997  9998  9999  node_id  \\\n",
       "0      0.000000   0.0   0.0   0.0   0.0   104447   \n",
       "1      0.000000   0.0   0.0   0.0   0.0    15858   \n",
       "2      0.000000   0.0   0.0   0.0   0.0   107156   \n",
       "3      0.000000   0.0   0.0   0.0   0.0    82077   \n",
       "4      0.000000   0.0   0.0   0.0   0.0    42436   \n",
       "...         ...   ...   ...   ...   ...      ...   \n",
       "53995  0.377827   0.0   0.0   0.0   0.0    82076   \n",
       "53996  0.000000   0.0   0.0   0.0   0.0     2630   \n",
       "53997  0.000000   0.0   0.0   0.0   0.0    29101   \n",
       "53998  0.000000   0.0   0.0   0.0   0.0    47784   \n",
       "53999  0.000000   0.0   0.0   0.0   0.0    45118   \n",
       "\n",
       "                                                   title  \\\n",
       "0      spreadsheets on the move an evaluation of mobi...   \n",
       "1      multi view metric learning for multi view vide...   \n",
       "2        big data analytics in future internet of things   \n",
       "3                 cryptographic hardening of d sequences   \n",
       "4      gesture based continuous authentication for we...   \n",
       "...                                                  ...   \n",
       "53995  co optimizing performance and memory footprint...   \n",
       "53996  fusionlane multi sensor fusion for lane markin...   \n",
       "53997  heatnet bridging the day night domain gap in s...   \n",
       "53998  vulnerabilities of connectionist ai applicatio...   \n",
       "53999  cross modal multi task learning for graphic re...   \n",
       "\n",
       "                                                abstract  label  \n",
       "0      The power of mobile devices has increased dram...      6  \n",
       "1      Traditional methods on video summarization are...     16  \n",
       "2      Current research on Internet of Things (IoT) m...      5  \n",
       "3      This paper shows how a one-way mapping using m...      4  \n",
       "4      We study the feasibility of touch gesture beha...      4  \n",
       "...                                                  ...    ...  \n",
       "53995  Cutting-edge embedded system applications, suc...      5  \n",
       "53996  It is a crucial step to achieve effective sema...     16  \n",
       "53997  The majority of learning-based semantic segmen...     16  \n",
       "53998  This article deals with the IT security of con...      4  \n",
       "53999  Face recognition of realistic visual images ha...     16  \n",
       "\n",
       "[54000 rows x 10004 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>9994</th>\n      <th>9995</th>\n      <th>9996</th>\n      <th>9997</th>\n      <th>9998</th>\n      <th>9999</th>\n      <th>node_id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>104447</td>\n      <td>spreadsheets on the move an evaluation of mobi...</td>\n      <td>The power of mobile devices has increased dram...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15858</td>\n      <td>multi view metric learning for multi view vide...</td>\n      <td>Traditional methods on video summarization are...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>107156</td>\n      <td>big data analytics in future internet of things</td>\n      <td>Current research on Internet of Things (IoT) m...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>82077</td>\n      <td>cryptographic hardening of d sequences</td>\n      <td>This paper shows how a one-way mapping using m...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>42436</td>\n      <td>gesture based continuous authentication for we...</td>\n      <td>We study the feasibility of touch gesture beha...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.377827</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>82076</td>\n      <td>co optimizing performance and memory footprint...</td>\n      <td>Cutting-edge embedded system applications, suc...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>53996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2630</td>\n      <td>fusionlane multi sensor fusion for lane markin...</td>\n      <td>It is a crucial step to achieve effective sema...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>53997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>29101</td>\n      <td>heatnet bridging the day night domain gap in s...</td>\n      <td>The majority of learning-based semantic segmen...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>53998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.682094</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>47784</td>\n      <td>vulnerabilities of connectionist ai applicatio...</td>\n      <td>This article deals with the IT security of con...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>53999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>45118</td>\n      <td>cross modal multi task learning for graphic re...</td>\n      <td>Face recognition of realistic visual images ha...</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>54000 rows × 10004 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame.sparse.from_spmatrix(sp), training_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       node_id                                              title  \\\n",
       "0       104447  spreadsheets on the move an evaluation of mobi...   \n",
       "1        15858  multi view metric learning for multi view vide...   \n",
       "2       107156    big data analytics in future internet of things   \n",
       "3        82077             cryptographic hardening of d sequences   \n",
       "4        42436  gesture based continuous authentication for we...   \n",
       "...        ...                                                ...   \n",
       "53995    82076  co optimizing performance and memory footprint...   \n",
       "53996     2630  fusionlane multi sensor fusion for lane markin...   \n",
       "53997    29101  heatnet bridging the day night domain gap in s...   \n",
       "53998    47784  vulnerabilities of connectionist ai applicatio...   \n",
       "53999    45118  cross modal multi task learning for graphic re...   \n",
       "\n",
       "                                                abstract  label  \n",
       "0      The power of mobile devices has increased dram...      6  \n",
       "1      Traditional methods on video summarization are...     16  \n",
       "2      Current research on Internet of Things (IoT) m...      5  \n",
       "3      This paper shows how a one-way mapping using m...      4  \n",
       "4      We study the feasibility of touch gesture beha...      4  \n",
       "...                                                  ...    ...  \n",
       "53995  Cutting-edge embedded system applications, suc...      5  \n",
       "53996  It is a crucial step to achieve effective sema...     16  \n",
       "53997  The majority of learning-based semantic segmen...     16  \n",
       "53998  This article deals with the IT security of con...      4  \n",
       "53999  Face recognition of realistic visual images ha...     16  \n",
       "\n",
       "[54000 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>node_id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>104447</td>\n      <td>spreadsheets on the move an evaluation of mobi...</td>\n      <td>The power of mobile devices has increased dram...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15858</td>\n      <td>multi view metric learning for multi view vide...</td>\n      <td>Traditional methods on video summarization are...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>107156</td>\n      <td>big data analytics in future internet of things</td>\n      <td>Current research on Internet of Things (IoT) m...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>82077</td>\n      <td>cryptographic hardening of d sequences</td>\n      <td>This paper shows how a one-way mapping using m...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>42436</td>\n      <td>gesture based continuous authentication for we...</td>\n      <td>We study the feasibility of touch gesture beha...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53995</th>\n      <td>82076</td>\n      <td>co optimizing performance and memory footprint...</td>\n      <td>Cutting-edge embedded system applications, suc...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>53996</th>\n      <td>2630</td>\n      <td>fusionlane multi sensor fusion for lane markin...</td>\n      <td>It is a crucial step to achieve effective sema...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>53997</th>\n      <td>29101</td>\n      <td>heatnet bridging the day night domain gap in s...</td>\n      <td>The majority of learning-based semantic segmen...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>53998</th>\n      <td>47784</td>\n      <td>vulnerabilities of connectionist ai applicatio...</td>\n      <td>This article deals with the IT security of con...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>53999</th>\n      <td>45118</td>\n      <td>cross modal multi task learning for graphic re...</td>\n      <td>Face recognition of realistic visual images ha...</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>54000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = training_set.title.tolist()\n",
    "abstract = training_set.abstract.tolist()\n",
    "concatenated = [*title, *abstract]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This article deals with the IT security of connectionist artificial intelligence (AI) applications, focusing on threats to integrity, one of the three IT security goals. Such threats are for instance most relevant in prominent AI computer vision applications. In order to present a holistic view on the IT security goal integrity, many additional aspects such as interpretability, robustness and documentation are taken into account. A comprehensive list of threats and possible mitigations is presented by reviewing the state-of-the-art literature. AI-specific vulnerabilities such as adversarial attacks and poisoning attacks as well as their AI-specific root causes are discussed in detail. Additionally and in contrast to former reviews, the whole AI supply chain is analysed with respect to vulnerabilities, including the planning, data acquisition, training, evaluation and operation phases. The discussion of mitigations is likewise not restricted to the level of the AI system itself but rather advocates viewing AI systems in the context of their supply chains and their embeddings in larger IT infrastructures and hardware devices. Based on this and the observation that adaptive attackers may circumvent any single published AI-specific defence to date, the article concludes that single protective measures are not sufficient but rather multiple measures on different levels have to be combined to achieve a minimum level of IT security for AI applications.',\n",
       " 'Face recognition of realistic visual images has been well studied and made a significant progress in the recent decade. Unlike the realistic visual images, the face recognition of the caricatures is far from the performance of the visual images. This is largely due to the extreme non-rigid distortions of the caricatures introduced by exaggerating the facial features to strengthen the characters. The heterogeneous modalities of the caricatures and the visual images result the caricature-visual face recognition is a cross-modal problem. In this paper, we propose a method to conduct caricature-visual face recognition via multi-task learning. Rather than the conventional multi-task learning with fixed weights of tasks, this work proposes an approach to learn the weights of tasks according to the importance of tasks. The proposed multi-task learning with dynamic tasks weights enables to appropriately train the hard task and easy task instead of being stuck in the over-training easy task as conventional methods. The experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning for cross-modal caricature-visual face recognition. The performances on the datasets CaVI and WebCaricature show the superiority over the state-of-art methods.']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "concatenated[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marc/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in concatenated[-2:]:\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    voc = [word for word in sentence if word not in stop_words]\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_vocabulary(training_set: pd.DataFrame):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer =  nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "    title = training_set.title.tolist()\n",
    "    abstract = training_set.abstract.tolist()\n",
    "    voc = []\n",
    "    # concatenate both lists\n",
    "    concat = [*title, *abstract]\n",
    "    for sentence in tqdm(concat):\n",
    "        # split sentence using tokenizer\n",
    "        sentence = tokenizer.tokenize(sentence.lower())\n",
    "        # removing stop words\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                voc.append(word)\n",
    "    # get unique words\n",
    "    voc = np.unique(voc)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 108000/108000 [00:10<00:00, 9872.58it/s]\n"
     ]
    }
   ],
   "source": [
    "voc = create_vocabulary(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['according', 'approach', 'appropriately', 'art', 'caricature',\n",
       "       'caricatures', 'cavi', 'characters', 'conduct', 'conventional',\n",
       "       'cross', 'datasets', 'decade', 'demonstrate', 'distortions', 'due',\n",
       "       'dynamic', 'easy', 'effectiveness', 'enables', 'exaggerating',\n",
       "       'experimental', 'extreme', 'face', 'facial', 'far', 'features',\n",
       "       'fixed', 'hard', 'heterogeneous', 'images', 'importance',\n",
       "       'instead', 'introduced', 'largely', 'learn', 'learning', 'made',\n",
       "       'method', 'methods', 'modal', 'modalities', 'multi', 'non',\n",
       "       'paper', 'performance', 'performances', 'problem', 'progress',\n",
       "       'propose', 'proposed', 'proposes', 'rather', 'realistic', 'recent',\n",
       "       'recognition', 'result', 'results', 'rigid', 'show', 'significant',\n",
       "       'state', 'strengthen', 'stuck', 'studied', 'superiority', 'task',\n",
       "       'tasks', 'train', 'training', 'unlike', 'via', 'visual',\n",
       "       'webcaricature', 'weights', 'well', 'work'], dtype='<U13')"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.modes import KeyedVectors\n",
    "path_word2vec = api.load(\"word2vec-google-news-300\", return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   title  \\\n",
       "0      spreadsheets on the move an evaluation of mobi...   \n",
       "1      multi view metric learning for multi view vide...   \n",
       "2        big data analytics in future internet of things   \n",
       "3                 cryptographic hardening of d sequences   \n",
       "4      gesture based continuous authentication for we...   \n",
       "...                                                  ...   \n",
       "53995  co optimizing performance and memory footprint...   \n",
       "53996  fusionlane multi sensor fusion for lane markin...   \n",
       "53997  heatnet bridging the day night domain gap in s...   \n",
       "53998  vulnerabilities of connectionist ai applicatio...   \n",
       "53999  cross modal multi task learning for graphic re...   \n",
       "\n",
       "                                                abstract  \n",
       "0      The power of mobile devices has increased dram...  \n",
       "1      Traditional methods on video summarization are...  \n",
       "2      Current research on Internet of Things (IoT) m...  \n",
       "3      This paper shows how a one-way mapping using m...  \n",
       "4      We study the feasibility of touch gesture beha...  \n",
       "...                                                  ...  \n",
       "53995  Cutting-edge embedded system applications, suc...  \n",
       "53996  It is a crucial step to achieve effective sema...  \n",
       "53997  The majority of learning-based semantic segmen...  \n",
       "53998  This article deals with the IT security of con...  \n",
       "53999  Face recognition of realistic visual images ha...  \n",
       "\n",
       "[54000 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spreadsheets on the move an evaluation of mobi...</td>\n      <td>The power of mobile devices has increased dram...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>multi view metric learning for multi view vide...</td>\n      <td>Traditional methods on video summarization are...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>big data analytics in future internet of things</td>\n      <td>Current research on Internet of Things (IoT) m...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cryptographic hardening of d sequences</td>\n      <td>This paper shows how a one-way mapping using m...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gesture based continuous authentication for we...</td>\n      <td>We study the feasibility of touch gesture beha...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53995</th>\n      <td>co optimizing performance and memory footprint...</td>\n      <td>Cutting-edge embedded system applications, suc...</td>\n    </tr>\n    <tr>\n      <th>53996</th>\n      <td>fusionlane multi sensor fusion for lane markin...</td>\n      <td>It is a crucial step to achieve effective sema...</td>\n    </tr>\n    <tr>\n      <th>53997</th>\n      <td>heatnet bridging the day night domain gap in s...</td>\n      <td>The majority of learning-based semantic segmen...</td>\n    </tr>\n    <tr>\n      <th>53998</th>\n      <td>vulnerabilities of connectionist ai applicatio...</td>\n      <td>This article deals with the IT security of con...</td>\n    </tr>\n    <tr>\n      <th>53999</th>\n      <td>cross modal multi task learning for graphic re...</td>\n      <td>Face recognition of realistic visual images ha...</td>\n    </tr>\n  </tbody>\n</table>\n<p>54000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_set.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-388aca337e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/provo/Python_projects/kaggle/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  }
 ]
}